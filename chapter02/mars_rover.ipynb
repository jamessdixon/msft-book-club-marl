{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d342de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41cee149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd66d2e",
   "metadata": {},
   "source": [
    "Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1875406",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = [\"START\", \"VOLCANO\", \"BASE\", \"SITE_A\", \"SITE_B\", \"IMMOBILE\"]\n",
    "ACTIONS = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "TERMINAL_STATES = {\"BASE\", \"IMMOBILE\", \"DESTROYED\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2f278a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARKOV DECISION PROCESS\n",
    "MDP = {\n",
    "    \"START\": {\n",
    "        \"LEFT\": [\n",
    "            {\"prob\": 0.9, \"next_state\": \"SITE_A\", \"reward\": -1},\n",
    "            {\"prob\": 0.1, \"next_state\": \"IMMOBILE\", \"reward\": -3},\n",
    "        ],\n",
    "        \"RIGHT\": [\n",
    "            {\"prob\": 0.5, \"next_state\": \"BASE\", \"reward\": 10},\n",
    "            {\"prob\": 0.5, \"next_state\": \"DESTROYED\", \"reward\": -10},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"SITE_A\": {\n",
    "        \"LEFT\": [\n",
    "            {\"prob\": 1.0, \"next_state\": \"START\", \"reward\": -1},\n",
    "        ],\n",
    "        \"RIGHT\": [\n",
    "            {\"prob\": 0.8, \"next_state\": \"SITE_B\", \"reward\": -1},\n",
    "            {\"prob\": 0.2, \"next_state\": \"IMMOBILE\", \"reward\": -3},\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"SITE_B\": {\n",
    "        \"LEFT\": [\n",
    "            {\"prob\": 1.0, \"next_state\": \"SITE_A\", \"reward\": -1},\n",
    "        ],\n",
    "        \"RIGHT\": [\n",
    "            {\"prob\": 1.0, \"next_state\": \"BASE\", \"reward\": 10},\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Terminal states (no actions)\n",
    "    \"BASE\": {},\n",
    "    \"IMMOBILE\": {},\n",
    "    \"DESTROYED\": {},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8b19255",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINAL_STATES = {s for s, a in MDP.items() if len(a) == 0}\n",
    "ALL_STATES = list(MDP.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9105f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env():\n",
    "    state = \"START\"\n",
    "    done = False\n",
    "    return state, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9743d",
   "metadata": {},
   "source": [
    "Game play - function to transition states in enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11b87b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(state, action):\n",
    "    if state in TERMINAL_STATES:\n",
    "        raise ValueError(\"Episode has terminated. Call reset_env().\")\n",
    "\n",
    "    if action not in MDP[state]:\n",
    "        raise ValueError(f\"Invalid action '{action}' for state '{state}'\")\n",
    "\n",
    "    outcomes = MDP[state][action]\n",
    "\n",
    "    r = random.random()\n",
    "    cumulative = 0.0\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        cumulative += outcome[\"prob\"]\n",
    "        if r <= cumulative:\n",
    "            next_state = outcome[\"next_state\"]\n",
    "            reward = outcome[\"reward\"]\n",
    "            done = next_state in TERMINAL_STATES\n",
    "            return next_state, reward, done\n",
    "\n",
    "    # Safety fallback (floating-point issues)\n",
    "    last = outcomes[-1]\n",
    "    next_state = last[\"next_state\"]\n",
    "    reward = last[\"reward\"]\n",
    "    done = next_state in TERMINAL_STATES\n",
    "    return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad0b86",
   "metadata": {},
   "source": [
    "Run An episode - apply the policy function to the environment and see the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b6a80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(policy_fn):\n",
    "    state, done = reset_env()\n",
    "    total_reward = 0\n",
    "    trajectory = []\n",
    "\n",
    "    while not done:\n",
    "        action = policy_fn(state)\n",
    "        next_state, reward, done = step_env(state, action)\n",
    "\n",
    "        trajectory.append((state, action, reward, next_state))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory, total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5041ec",
   "metadata": {},
   "source": [
    "Create the polices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba913c",
   "metadata": {},
   "source": [
    "#this is a 'policy' where the rover looks at all states and determines the optimal path\n",
    "#there is no reinforcement learning, jsut a straight calculation\n",
    "#this becomes the greedy policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d8947b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_values():\n",
    "    return {state: 0.0 for state in MDP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3355579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma=0.95, theta=1e-6):\n",
    "    V = initialize_values()\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for state, actions in mdp.items():\n",
    "\n",
    "            # Skip terminal states\n",
    "            if not actions:\n",
    "                continue\n",
    "\n",
    "            best_value = float(\"-inf\")\n",
    "\n",
    "            for action, outcomes in actions.items():\n",
    "                action_value = 0\n",
    "\n",
    "                for o in outcomes:\n",
    "                    prob = o[\"prob\"]\n",
    "                    reward = o[\"reward\"]\n",
    "                    next_state = o[\"next_state\"]\n",
    "\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "                best_value = max(best_value, action_value)\n",
    "\n",
    "            delta = max(delta, abs(V[state] - best_value))\n",
    "            V[state] = best_value\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00f62e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimal_policy(mdp, V, gamma=0.95):\n",
    "    policy = {}\n",
    "\n",
    "    for state, actions in mdp.items():\n",
    "        if not actions:\n",
    "            continue\n",
    "\n",
    "        best_action = None\n",
    "        best_value = float(\"-inf\")\n",
    "\n",
    "        for action, outcomes in actions.items():\n",
    "            action_value = 0\n",
    "\n",
    "            for o in outcomes:\n",
    "                action_value += o[\"prob\"] * (\n",
    "                    o[\"reward\"] + gamma * V[o[\"next_state\"]]\n",
    "                )\n",
    "\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "\n",
    "        policy[state] = best_action\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34fee720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "START     : 4.101\n",
      "SITE_A    : 6.200\n",
      "SITE_B    : 10.000\n",
      "BASE      : 0.000\n",
      "IMMOBILE  : 0.000\n",
      "DESTROYED : 0.000\n",
      "\n",
      "Optimal Policy:\n",
      "START      -> LEFT\n",
      "SITE_A     -> RIGHT\n",
      "SITE_B     -> RIGHT\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95   # how much the rover values future rewards\n",
    "\n",
    "V_star = value_iteration(MDP, GAMMA)\n",
    "optimal_policy = extract_optimal_policy(MDP, V_star, GAMMA)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "for s, v in V_star.items():\n",
    "    print(f\"{s:10s}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s, a in optimal_policy.items():\n",
    "    print(f\"{s:10s} -> {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b5b81",
   "metadata": {},
   "source": [
    "Create deterministic policy based on table above\n",
    "This is not RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ba0534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state):\n",
    "    if state == \"START\":\n",
    "        return \"LEFT\"\n",
    "    if state in [\"SITE_A\", \"SITE_B\"]:\n",
    "        return \"RIGHT\"\n",
    "    raise ValueError(\"No action available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a31bb",
   "metadata": {},
   "source": [
    "Create the one step look ahead policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0d7ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead_policy(state):\n",
    "    actions = MDP[state]\n",
    "\n",
    "    if not actions:\n",
    "        raise ValueError(f\"No actions available from terminal state {state}\")\n",
    "\n",
    "    best_action = None\n",
    "    best_ev = float(\"-inf\")\n",
    "\n",
    "    for action, outcomes in actions.items():\n",
    "        ev = 0.0\n",
    "        for o in outcomes:\n",
    "            ev += o[\"prob\"] * o[\"reward\"]\n",
    "\n",
    "        if ev > best_ev:\n",
    "            best_ev = ev\n",
    "            best_action = action\n",
    "\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdc41e",
   "metadata": {},
   "source": [
    "Create 100% random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adaa07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    actions = list(MDP[state].keys())\n",
    "    return random.choice(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec5145",
   "metadata": {},
   "source": [
    "Create RL Policy by running 10000 times and seeing the results of each action\n",
    "Then chaning the path based on past results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d6d080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "\n",
    "for state, actions in MDP.items():\n",
    "    for action in actions:\n",
    "        Q[(state, action)] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc060f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1      # learning rate\n",
    "gamma = 0.95     # future reward importance\n",
    "epsilon = 0.1    # exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "743905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    actions = list(MDP[state].keys())\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)  # explore\n",
    "\n",
    "    return max(actions, key=lambda a: Q[(state, a)])  # exploit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d1319f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(episodes=10_000):\n",
    "    for _ in range(episodes):\n",
    "        state, done = reset_env()\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state)\n",
    "            next_state, reward, done = step_env(state, action)\n",
    "\n",
    "            if next_state in MDP and MDP[next_state]:\n",
    "                future = max(Q[(next_state, a)] for a in MDP[next_state])\n",
    "            else:\n",
    "                future = 0  # terminal state\n",
    "\n",
    "            Q[(state, action)] += alpha * (\n",
    "                reward + gamma * future - Q[(state, action)]\n",
    "            )\n",
    "\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "813c8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5692c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learned_policy(state):\n",
    "    actions = list(MDP[state].keys())\n",
    "    return max(actions, key=lambda a: Q[(state, a)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b26b1",
   "metadata": {},
   "source": [
    "Run Policys one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34a40d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('START', 'LEFT', -1, 'SITE_A')\n",
      "('SITE_A', 'RIGHT', -1, 'SITE_B')\n",
      "('SITE_B', 'RIGHT', 10, 'BASE')\n",
      "Total reward: 8\n"
     ]
    }
   ],
   "source": [
    "trajectory, total_reward = run_episode(greedy_policy)\n",
    "\n",
    "for step in trajectory:\n",
    "    print(step)\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37ad9e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory:\n",
      "('START', 'RIGHT', -10, 'DESTROYED')\n",
      "Total reward: -10\n"
     ]
    }
   ],
   "source": [
    "trajectory, total_reward = run_episode(one_step_lookahead_policy)\n",
    "\n",
    "print(\"Trajectory:\")\n",
    "for step in trajectory:\n",
    "    print(step)\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67ab7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory:\n",
      "('START', 'LEFT', -3, 'IMMOBILE')\n",
      "Total reward: -3\n"
     ]
    }
   ],
   "source": [
    "trajectory, total_reward = run_episode(learned_policy)\n",
    "\n",
    "print(\"Trajectory:\")\n",
    "for step in trajectory:\n",
    "    print(step)\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687f227",
   "metadata": {},
   "source": [
    "Run policies 10,000 times (Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d012a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy_fn, episodes=10000):\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        _, total = run_episode(policy_fn)\n",
    "        rewards.append(total)\n",
    "    return sum(rewards) / len(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3aa28ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy avg reward: 4.6476\n",
      "Random policy avg reward: -0.4732\n",
      "One step lookahead policy avg reward: -0.166\n",
      "Learned policy avg reward: 0.14\n"
     ]
    }
   ],
   "source": [
    "print(\"Greedy policy avg reward:\", evaluate_policy(greedy_policy))\n",
    "print(\"Random policy avg reward:\", evaluate_policy(random_policy))\n",
    "print(\"One step lookahead policy avg reward:\", evaluate_policy(one_step_lookahead_policy))\n",
    "print(\"Learned policy avg reward:\", evaluate_policy(one_step_lookahead_policy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c7a97",
   "metadata": {},
   "source": [
    "Print out the network graph because I love graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mdp_graph(MDP):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for state, actions in MDP.items():\n",
    "        if not actions:  # terminal state\n",
    "            G.add_node(state)\n",
    "            continue\n",
    "\n",
    "        for action, outcomes in actions.items():\n",
    "            for o in outcomes:\n",
    "                label = f\"{action}\\nP={o['prob']}, R={o['reward']}\"\n",
    "                G.add_edge(state, o[\"next_state\"], label=label)\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=2800,\n",
    "        node_color=\"lightgray\",\n",
    "        font_size=10,\n",
    "        arrows=True\n",
    "    )\n",
    "\n",
    "    edge_labels = nx.get_edge_attributes(G, \"label\")\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    plt.title(\"Mars Rover MDP\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mdp_graph(MDP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
